{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sps\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from helpers import *\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso with SGD and Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization problem for sparse least squares linear regression (also known as the Lasso) is given by\n",
    "\\begin{equation}\\label{eq:lasso}\n",
    "  \\min_{x \\in R^n} \\   \\|Ax - b\\|^2 + \\lambda \\|x\\|_1\n",
    "\\end{equation}\n",
    "for some regularization parameter $\\lambda>0$, where $A\\in R^{d\\times n}$ is the given data matrix, and $b\\in R^d$ is the given ``measurement'' vector. The $\\|.\\|_1$-norm is the sum of absolute values of coordinates of its argument.\n",
    "\n",
    "The goal of this exercise is to implement stochastic (sub)gradient descent (SGD) as well as coordinate descent on this objective function, where both algorithms will work based on the $x$ variable vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 11\n",
    "A = np.random.normal(size = (d, 10))\n",
    "b = np.random.uniform(1, 0, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use a real dataset in LibSVM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, b = load_svmlight_file(\"data/ionosphere.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_function(A, b, reg_coef, alpha):\n",
    "    return 0.5 * np.linalg.norm(A.dot(alpha) - b) ** 2 + reg_coef * np.sum(np.abs(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient_lasso(A, b, reg_coef, alpha):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    err = b - A.dot(alpha)\n",
    "    gradient = - A.T.dot(err) + reg_coef * np.sign(alpha) * A.shape[0]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_lasso(A, b, reg_coef, gamma, batch_size=1, max_iter=1000, trace=False):\n",
    "    history = defaultdict(list) if trace else None\n",
    "    \n",
    "    num_data_points, num_features = np.shape(A)\n",
    "    alpha_t = np.zeros(num_features)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for current_iter in range(0, max_iter):\n",
    "        if trace:\n",
    "            history['time'].append(time.time() - start_time)\n",
    "            history['objective_function'].append(lasso_function(A, b, reg_coef, alpha_t))\n",
    "        ...\n",
    "        \n",
    "    return alpha_t, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, history = stochastic_gradient_descent_lasso(A, b, 0.1, 0.005, max_iter=5000, trace=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(history['objective_function']) # log scale\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"objective value\")\n",
    "plt.title(\"objective value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Coordinate Descent for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(internal, reg_coef, current_feature_norm):\n",
    "    if internal > reg_coef:\n",
    "        return (internal - reg_coef) / (current_feature_norm ** 2)\n",
    "    elif internal < - reg_coef:\n",
    "        return (internal + reg_coef) / (current_feature_norm ** 2)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent_lasso(A, b, reg_coef, max_iter=1000, trace=False, is_check=False,\n",
    "                             check_epsilon=1e-1):\n",
    "    history = defaultdict(list) if trace else None\n",
    "\n",
    "    num_features = np.shape(A)[1]\n",
    "    alpha_t = np.zeros(num_features)\n",
    "    residual = np.copy(b)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for current_iter in range(0, max_iter):\n",
    "        if trace:\n",
    "            history['time'].append(time.time() - start_time)\n",
    "            history['objective_function'].append(lasso_function(A, b, reg_coef, alpha_t))\n",
    "        i = np.random.randint(0, num_features)\n",
    "        if sps.issparse(A):\n",
    "            current_feature = np.array(A[:, i].todense())\n",
    "        else:\n",
    "            current_feature = np.array(A[:, i])\n",
    "\n",
    "        current_feature_norm = np.linalg.norm(current_feature)\n",
    "        if current_feature_norm == 0:\n",
    "            alpha_t[i] = 0\n",
    "            continue\n",
    "        \n",
    "        ...\n",
    "\n",
    "        new_value = soft_threshold( ... , reg_coef, current_feature_norm)\n",
    "        residual += (current_feature * (alpha_t[i] - new_value)).reshape(residual.shape)\n",
    "        alpha_t[i] = np.copy(new_value)\n",
    "\n",
    "        if is_check:  # implement an optional check if a slightly smaller or larger step on \n",
    "                      # the current coordinate would indeed make the function value worse\n",
    "            \n",
    "    return alpha_t, residual, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally run the optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, residual, history = coordinate_descent_lasso(A, b, 0.1, max_iter=1000, trace=True,\n",
    "                                                  is_check=True, check_epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(history['objective_function'])\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"objective value\")\n",
    "plt.title(\"objective value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "## Classification Using SVM\n",
    "Load dataset. We will re-use the CERN dataset from project 1 of the ML course, available from https://inclass.kaggle.com/c/epfml-project-1/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original optimization problem for the Support Vector Machine (SVM) is given by\n",
    "\\begin{equation}\\label{eq:primal}\n",
    "  \\min_{w \\in R^d} \\  \\sum_{i=1}^n \\ell(y_i A_i^\\top w) + \\frac\\lambda2 \\|w\\|^2\n",
    "\\end{equation}\n",
    "where $\\ell : R\\rightarrow R$, $\\ell(z) := \\max\\{0,1-z\\}$ is the hinge loss function.\n",
    "Here for any $i$, $1\\le i\\le n$, the vector $A_i\\in R^d$ is the $i$-th data example, and $y_i\\in\\{\\pm1\\}$ is the corresponding label.\n",
    "  \n",
    "The dual optimization problem for the SVM is given by \n",
    "\\begin{equation}\\label{eq:dual}\n",
    " \\max_{\\boldsymbol{\\alpha} \\in R^n } \\  \\alpha^\\top\\boldsymbol{1} - \\tfrac1{2\\lambda} \\alpha^\\top Y A^\\top AY\\alpha\n",
    " \\text{    such that    $0\\le \\alpha_i \\le 1  \\ \\forall i$}\n",
    "\\end{equation}\n",
    "where $Y := \\mathop{diag}(y)$, and $A\\in R^{d\\times n}$ again collects all $n$ data examples as its columns. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "\n",
    "y, A, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)\n",
    "print(y.shape, A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare cost and prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_primal_objective(y, A, w, lambda_):\n",
    "    \"\"\"compute the full cost (the primal objective), that is loss plus regularizer.\n",
    "    A: the full dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, A, w):\n",
    "    \"\"\"compute the training accuracy on the training set (can be called for test set as well).\n",
    "    A: the full dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent (Ascent) for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the closed-form update for the i-th variable alpha, in the dual optimization problem, given alpha and the current corresponding w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coordinate_update(y, A, lambda_, alpha, w, i):\n",
    "    \"\"\"compute a coordinate update (closed form) for coordinate i.\n",
    "    A: the dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_examples)\n",
    "    n: the coordinate to be updated\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    # calculate the update of coordinate at index=n.\n",
    "    a_i, y_i = A[i], y[i]\n",
    "    old_alpha_i = np.copy(alpha[i])\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    return w, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dual_objective(y, A, w, alpha, lambda_):\n",
    "    \"\"\"calculate the objective for the dual problem.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent_for_svm_demo(y, A):\n",
    "    max_iter = 100000\n",
    "    lambda_ = 0.01\n",
    "\n",
    "    num_examples, num_features = A.shape\n",
    "    w = np.zeros(num_features)\n",
    "    alpha = np.zeros(num_examples)\n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        # i = sample one data point uniformly at random from the columns of A\n",
    "        i = random.randint(0,num_examples-1)\n",
    "        \n",
    "        w, alpha = calculate_coordinate_update(y, A, lambda_, alpha, w, i)\n",
    "            \n",
    "        if it % 10000 == 0:\n",
    "            # primal objective\n",
    "            primal_value = calculate_primal_objective(y, A, w, lambda_)\n",
    "            # dual objective\n",
    "            dual_value = calculate_dual_objective(y, A, w, alpha, lambda_)\n",
    "            # primal dual gap\n",
    "            duality_gap = primal_value - dual_value\n",
    "            print('iteration=%i, primal:%.5f, dual:%.5f, gap:%.5f'%(\n",
    "                    it, primal_value, dual_value, duality_gap))\n",
    "    print(\"training accuracy = {l}\".format(l=calculate_accuracy(y, A, w)))\n",
    "\n",
    "coordinate_descent_for_svm_demo(y, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
